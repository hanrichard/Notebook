### s3
For objects larger than 100 megabytes use the Multipart Upload capability.
Amazon S3 automatically scales to high request rates.
For example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket.

An S3 bucket is region specific.

aCLs – define permissions to access the object.

- read after write consistency for new object
- eventual consistency OVERWRITE put and delete files
- 11 9s

classes:
- s3 standard: million seconds
- s3 ia: infrequent accessed: charge to retrieve: million seconds
- s3 one zone ia: million seconds
- s3 intelligent tiering: machine learnng: million seconds
- glacier: configure retrive time minute - hours: 1 min - 12 hours
- glacier deep: retrive 12 hours 

- file size: 0-5TB

bucket: acl ( access control lists )/ bucket policy

encryption: 
ssl: Secure Sockets Layer 
tls: Transport Layer Security

sse-s3: s3 manages keys
sse-kms: s3 + customer
sse-c: customer
client side

life cycle: transition -> expire (current and previous version)

scp: service control policies

### bucket share:
- use bucket policy and IAM
- bucket ACL and IAM
- cross-account IAM

### performance 
- byte range fetch
- prefix
- multi upload

### signed url vs cookies - attach a policy - url epxiraton, etc
- one url - one file
- cookie - multi files
- ec2 -> cloudfront
- s3 -> s3 signed url

### replication
- versioning enable
- delete/ delete marker not replicated 

edge location: read and write

### snowball
- 50tb
- 100tb, computing, lambda
- import and export s3

### gateway ???
- file gateway: nfs & smb - flat files
- volumn gateway: isci
  - stored volumns: entire dataset and asyc back on S3
  - cached volumns: entire dataset on S3 and frequent accessed data cache on site
- tape gateway

### athene
sql s3 as database

pii: personal identificable information

### macie
use ai to analyse pii


### note
- largest file can be 5Gb\
- larger than 100mb, multi part upload
- read after write consistency for put NEW objects
- eventual consistency overwrite on puts and deletes
- an s3 in region specific
- standards-based REST web interfaces 

### byte-range

### cloud front
- apex dns name on cloud front
- support wild card of cname
- regional cache location < edge locaitons

- resrcit access to content using signed cookies or signed url
- restrict access to objects in s3
- OAI - they can not access content, but connect tho via cloutfront

### notes
- Amazon S3 is a distributed architecture and objects are redundantly stored on multiple devices across multiple facilities (AZs) in an Amazon S3 region.
- Files can be from 0 bytes to 5TB.
- The largest object that can be uploaded in a single PUT is 5 gigabytes.
- An S3 bucket is region specific. !!!!

- Access Control Lists (ACLs) – control permissions access to the bucket.
- Bucket Policies – control access to the bucket.
- Cross Origin Resource Sharing (CORS).

Analytic 
- data lake concept: athena, quicksight, redshift spectrum
- IOT STREAMING DATA: knises firehole
- machine learning, ai - rekonition, lex, mxnet
- storage class Analytic: s3 management analytic


### efs
- The VPC of the connecting instance must have DNS hostnames enabled.
- Can be mounted from on-premises systems ONLY if using Direct Connect or a VPN connection.
- Alternatively, use the EFS File Sync agent.

### share s3 bucket accross accounts
- bucket policies and IAM role - programmtic only - accross entire bucket
- bucket ACLs and IAM role - programmtic only - files
- cross account IAM role - console and programmtic 

### pre-signed urls
- Pre-signed URLs can be used to provide temporary access to a specific object to those who do not have AWS credentials.
- By default, all objects are private and can only be accessed by the owner.
- To share an object, you can either make it public or generate a pre-signed URL. Expiration date and time must be configured.
- These can be generated using SDKs for Java and .Net and AWS explorer for Visual Studio. Can be used for downloading and uploading S3 objects.

### You can setup access control to your buckets using:
• Bucket policies.
• Access Control Lists.

- By using an OAI you can restrict users so they cannot access the content directly using the S3 URL, they must connect via CloudFront.

### test
- Expedited —Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archives (250 MB+), data accessed using Expedited retrievals are typically made available within 1–5 minutes. Provisioned Capacity ensures that retrieval capacity for Expedited retrievals is available when you need it.
- Standard —Standard retrievals allow you to access any of your archives within several hours. Standard retrievals typically complete within 3–5 hours. This is the default option for retrieval requests that do not specify the retrieval option.
- Bulk —Bulk retrievals are S3 Glacier’s lowest-cost retrieval option, which you can use to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5–12 hours.

- Amazon EFS provides high-performance, secure access for thousands of connections to a shared file system using a traditional file permissions model, file locking, and hierarchical directory structure via the NFSv4 protocol.

- It allows you to simultaneously share files between multiple Amazon EC2 instances across multiple AZs, regions, VPCs, and accounts as well as on-premises servers via AWS Direct Connect or AWS VPN.

- This is ideal for your business applications that need to share a common data source. For application workloads with many instances accessing the same set of files, Amazon EFS provides strong data consistency helping to ensure that any file read will reflect the last write of the file.

- INCORRECT: “Use Amazon S3 Transfer acceleration” is incorrect. Amazon S3 Transfer Acceleration is used for speeding up uploads of data to Amazon S3 by using the CloudFront network. It is not used for downloading data.

- INCORRECT: “Use Amazon CloudFront to cache the content” is incorrect. Amazon CloudFront is used for caching content closer to users. In this case the EC2 instance needs to access the data so CloudFront is not a good solution (the edge location used by CloudFront may not be closer than the EC2 instance is to the S3 endpoint.

- INCORRECT: “Use AWS Global Accelerator” is incorrect. AWS Global Accelerator is used for improving availability and performance for Amazon EC2 instances or Elastic Load Balancers (ALB and NLB). It is not used for improving Amazon S3 performance.

- All EBS types and all instance families support encryption but not all instance types support encryption. There is no direct way to change the encryption state of a volume. Data in transit between an instance and an encrypted volume is also encrypted.
- The possible values are ok, impaired, warning, or insufficient-data. If all checks pass, the overall status of the volume is ok. If the check fails, the overall status is impaired. If the status is insufficient-data, then the checks may still be taking place on your volume at the time.

- “Store the original images in STANDARD for 30 days, transition to DEEP_ARCHIVE for 90 days, then expire the data” is incorrect. DEEP_ARCHIVE has a minimum storage duration of 180 days

- All EBStypes support encryption and all instance families now support encryption.
- Not all instancetypes support encryption.
- Data in transit between an instance and an encrypted volume is also encrypted (data is encrypted in trans.
- You can have encrypted an unencrypted EBS volumes attached to an instance at the same time.
- Snapshots of encrypted volumes are encrypted automatically.
- EBS volumes restored from encrypted snapshots are encrypted automatically.
- EBS volumes created from encrypted snapshots are also encrypted.
- all EBS volume types support encryption.
